{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de94c33-955d-4dc8-bfb3-ef9d4b068243",
   "metadata": {},
   "source": [
    "# BDS Final Project\n",
    "Ethan Assefa, Thomas Burrell, Tatev Gomtsyan\n",
    "\n",
    "### Loads in and reads data in parallel across all the Ray workers:\n",
    "Preprocessing first to ensure empty strings dont cause problems, then read in full datasets.\n",
    "- Done for each of the three categories we are interested in (Appliances, All Beauty, and Video Games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ce4830-5b69-4e07-8b1d-9a15c37194fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 606201 texts from Appliances.jsonl.gz into Appliances_full_texts.\n",
      "Loaded 196645 texts from All_Beauty.jsonl.gz into All_Beauty_full_texts.\n",
      "Loaded 863026 texts from Musical_Instruments.jsonl.gz into Musical_Instruments_full_texts.\n",
      "Loaded 1370708 texts from Video_Games.jsonl.gz into Video_Games_full_texts.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import random  # Import random module for sampling\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "try:\n",
    "    # Official way to initialize Ray with memory management\n",
    "    ray.init(address='ray://172.31.28.176:10001')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Ray: {e}\")\n",
    "\n",
    "@ray.remote(num_cpus=2)\n",
    "def load_data(file_path, start, end):\n",
    "    texts = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        f.seek(start)\n",
    "        if start != 0:\n",
    "            f.readline()  # Skip partial line\n",
    "        while f.tell() < end:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                json_data = json.loads(line)\n",
    "                texts.append(json_data['text'])\n",
    "    return texts\n",
    "\n",
    "def read_data(file_path, num_workers, sample_fraction=0.1):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    chunk_size = file_size // num_workers\n",
    "    futures = []\n",
    "    for i in range(num_workers):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size if i != num_workers - 1 else file_size\n",
    "        futures.append(load_data.remote(file_path, start, end))\n",
    "    \n",
    "    all_texts = []\n",
    "    for future in ray.get(futures):\n",
    "        all_texts.extend(future)\n",
    "\n",
    "    # Sample a fraction of the data randomly\n",
    "    sampled_texts = random.sample(all_texts, int(len(all_texts) * sample_fraction))\n",
    "    return sampled_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = ['Appliances.jsonl.gz', 'All_Beauty.jsonl.gz', 'Musical_Instruments.jsonl.gz', 'Video_Games.jsonl.gz']\n",
    "    num_workers = 7  # Define the number of Ray workers to use\n",
    "\n",
    "    # Cycle through the given files and pull texts\n",
    "    for file_path in file_paths:\n",
    "        texts = read_data(file_path, num_workers, sample_fraction=1)  # Read data from each file\n",
    "        var_name = file_path.split('.')[0] + \"_full_texts\"  # Create variable name based on the file name\n",
    "        globals()[var_name] = texts  # Assign texts to a dynamically named variable\n",
    "        print(f\"Loaded {len(texts)} texts from {file_path} into {var_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba39be-c8bc-48b1-b87d-aa4a5b421f57",
   "metadata": {},
   "source": [
    "### Test Run of Sentiment Analysis\n",
    "Run small subset test on \"local\" head node without distributing across workers first to make sure there is no problem with the sentiment analysisx logic of our code:\n",
    "- Conducted on the appliances dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245dd98a-a46e-42c4-9fde-6149c779444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"They close, won't deteriorate like many other hard plastic types. I read in reviews a tinkling noise, but i haven't noticed and been a year.\", 'POSITIVE', 0.9998600482940674), (\"K-cups are such a burden on the environment. In fact, the inventor of the Keurig. The inventor of the K-cup, John Sylvan said, &quot;I feel bad sometimes that I ever did it.&quot; Now, there is a way to lessen your burden on the environment by using reusable K-cups! These are solid, nice, pretty and reusable cups that can easily be filled with coffee, tea or the drink of your choice. They work just like regular K-cups except you don't throw them away after a use. I feel so much better about my coffee consumption now that I don't have to throw them away. I highly recommend this product due to the green factor.<br /><br />I received this item at a discount in exchange for my honest review.\", 'POSITIVE', 0.9998317956924438), ('Great deal.', 'POSITIVE', 0.9862760305404663), ('I love this because the ice cubes are a solid cube but it is not self cleaning. Ive had it a couple months and it doesnt recycle the water that melts so you have to drain and then clean it<br /><br />It stopped working today', 'NEGATIVE', 0.9999935626983643), ('Samsung wanted $130 for this part so I looked around and found this one for half the price. I was worried a bit about quality but when it arrived it looked perfect. My Samsung is a WF45K6200AW/A2 model. My Father-in-law helped us install it and he’s genius at that kind of stuff so I can’t say too much about ease of installing, though I imagine if we had tried to do it without him we would still be out in the garage three days later with a non-operational washer surrounded by piles of dirty clothes. Anyway, excellent value and the part was a perfect fit for our washer. The added plus is we now have a clean gasket again. The old one was absolutely DISGUSTING. No matter how much we cleaned it we could never get it completely clean.', 'POSITIVE', 0.9456067681312561), (\"My original drip pans are twice as heavy duty - these are a bit flimsy and thinner - disappointed in that.  I guess they prefer to have a disposable society instead of real quality long lasting products that won't fill the land fills.  Most of the eco-philosophy is BS - furniture, appliances and most of the things we purchase have become disposable - quality went out the window\", 'NEGATIVE', 0.9999969005584717), ('The pro arrived exactly when he said he would. He found the problem quickly and completed the job in no time. He was very personable , and the price was lower than I had expected.', 'POSITIVE', 0.9995574355125427), ('The clamp did NOT create a seal and caused $2,000 worth of drywall and plumbing repair.', 'NEGATIVE', 0.9999706745147705), ('worked as expected and cheaper than else where thank you', 'POSITIVE', 0.9978140592575073), ('Item arrived in a padded envelope. Was damaged and not usable. Found a much heavier gauge one at the local hardware store for one quarter the price. This item is almost paper thin. Returned for refund.', 'NEGATIVE', 0.9999750852584839)]\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "def test_local_sentiment_analysis(texts, model_path):\n",
    "    classifier = TextClassifier.load(model_path)\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        sentence = Sentence(text)\n",
    "        try:\n",
    "            classifier.predict(sentence)\n",
    "            results.append((sentence.text, sentence.labels[0].value, sentence.labels[0].score))\n",
    "        except Exception as e:\n",
    "            results.append((sentence.text, 'Error', str(e)))\n",
    "    return results\n",
    "\n",
    "# Example usage with a small subset\n",
    "sample_texts = Appliances_full_texts[:10]\n",
    "model_path = os.path.expanduser('~/.flair/models/sentiment-en-mix-distillbert_4.pt')\n",
    "local_results = test_local_sentiment_analysis(sample_texts, model_path)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639ae89-0ffc-4fac-ab68-207354491f7e",
   "metadata": {},
   "source": [
    "### Take subset of full data \n",
    "We decided to take a percentage of each full dataset for this exercise, with the knowledge that the infrastructure can be scaled to handle the entire dataset if we wish.\n",
    "- Took 10% for each of the first three categories we are interested in (Appliances, All Beauty, and Musical Instruments)\n",
    "- Took 5% for the last and largest category (Video Games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0666aab6-b4fd-4b1f-8210-750800a24611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60620 texts from Appliances.jsonl.gz into Appliances_sampled_texts.\n",
      "Loaded 19664 texts from All_Beauty.jsonl.gz into All_Beauty_sampled_texts.\n",
      "Loaded 86302 texts from Musical_Instruments.jsonl.gz into Musical_Instruments_sampled_texts.\n",
      "Loaded 68535 texts from Video_Games.jsonl.gz into Video_Games_sampled_texts.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import random  # Import random module for sampling\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "try:\n",
    "    # Official way to initialize Ray with memory management\n",
    "    ray.init(address='ray://172.31.28.176:10001')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Ray: {e}\")\n",
    "\n",
    "@ray.remote\n",
    "def load_data(file_path, start, end):\n",
    "    texts = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        f.seek(start)\n",
    "        if start != 0:\n",
    "            f.readline()  # Skip partial line\n",
    "        while f.tell() < end:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                json_data = json.loads(line)\n",
    "                texts.append(json_data['text'])\n",
    "    return texts\n",
    "\n",
    "def read_data(file_path, num_workers, sample_fraction=0.1):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    chunk_size = file_size // num_workers\n",
    "    futures = []\n",
    "    for i in range(num_workers):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size if i != num_workers - 1 else file_size\n",
    "        futures.append(load_data.remote(file_path, start, end))\n",
    "    \n",
    "    all_texts = []\n",
    "    for future in ray.get(futures):\n",
    "        all_texts.extend(future)\n",
    "\n",
    "    # Sample a fraction of the data randomly\n",
    "    sampled_texts = random.sample(all_texts, int(len(all_texts) * sample_fraction))\n",
    "    return sampled_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configurations = [\n",
    "        {'file_path': 'Appliances.jsonl.gz', 'sample_fraction': 0.1},\n",
    "        {'file_path': 'All_Beauty.jsonl.gz', 'sample_fraction': 0.1},\n",
    "        {'file_path': 'Musical_Instruments.jsonl.gz', 'sample_fraction': 0.1},\n",
    "        {'file_path': 'Video_Games.jsonl.gz', 'sample_fraction': 0.05}\n",
    "    ]\n",
    "    #file_paths = ['Appliances.jsonl.gz', 'All_Beauty.jsonl.gz', 'Musical_Instruments.jsonl.gz', 'Video_Games.jsonl.gz']\n",
    "    num_workers = 7  # Define the number of Ray workers to use\n",
    "\n",
    "    # Cycle through the given files and pull texts\n",
    "    for config in configurations:\n",
    "        texts = read_data(config['file_path'], num_workers, sample_fraction=config['sample_fraction'])  # Read data from each file\n",
    "        var_name = config['file_path'].split('.')[0] + \"_sampled_texts\"  # Create variable name based on the file name\n",
    "        globals()[var_name] = texts  # Assign texts to a dynamically named variable\n",
    "        print(f\"Loaded {len(texts)} texts from {config['file_path']} into {var_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54bb09-718a-4518-9bbb-ccaa35fa4f77",
   "metadata": {},
   "source": [
    "## Full System\n",
    "We begin with 7 EC2 Instances (T3.Large), each with 2 CPUs. The system capabilities are laid out here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0c4093-a505-4cbe-a351-35f2152a0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cluster consists of\n",
      "    7 nodes in total\n",
      "    14.0 CPU resources in total\n",
      "    35.468619922176 Memory resources in total\n",
      "    15.508581158705056 Object store memory resources in total\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "try:\n",
    "    # Official way to initialize Ray with memory management\n",
    "    ray.init(address='ray://172.31.28.176:10001')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Ray: {e}\")\n",
    "\n",
    "# Print the status of the Ray cluster.\n",
    "print('''This cluster consists of\n",
    "    {} nodes in total\n",
    "    {} CPU resources in total\n",
    "    {} Memory resources in total\n",
    "    {} Object store memory resources in total\n",
    "    '''.format(len(ray.nodes()), ray.cluster_resources()['CPU'],\n",
    "           ray.cluster_resources()['memory'] / (1024*1024*1024),\n",
    "           ray.cluster_resources()['object_store_memory'] / (1024*1024*1024)))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b820c-d4fe-41c9-976b-1d525ccf2f53",
   "metadata": {},
   "source": [
    "## Run sentiment analysis in parallel across the Ray workers for Each Review Category:\n",
    "Four configurations each:\n",
    "1. Number of Workers: 2 | Number of CPUs per Worker: 1 | Batch Size: 100\n",
    "3. Number of Workers: 7 | Number of CPUs per Worker: 2 | Batch Size: 100\n",
    "4. Number of Workers: 7 | Number of CPUs per Worker: 2 | Batch Size: 200\n",
    "5. Number of Workers: 14 | Number of CPUs per Worker: 1 | Batch Size: 100\n",
    " \n",
    "### Appliances Amazon Reviews\n",
    "We begin with the Appliances sampled data, running the sentiment analysis six times with each system configuration.\n",
    "- A log of events and warnings is kept to ensure we are aware of any trouble when running (`senti_analy_appliances.log`)\n",
    "- Timers are set to provide the runtimes for each system configuration (time data saved to `appliances_timing.csv`)\n",
    "  - Total time, time per batch, average time per text\n",
    "- Results from the sentiment analysis on each review text are collected (saved to `appliances_results.csv`)\n",
    "  - Text content, sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b21d96c-5106-416c-a834-8b0722957b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60598 sentiment analyses with configuration: Workers=2, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'appliances_timing.csv'.\n",
      "Processed 60859 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=100\n",
      "Timing data appended to 'appliances_timing.csv'.\n",
      "Processed 61459 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=200\n",
      "Timing data appended to 'appliances_timing.csv'.\n",
      "Processed 61531 sentiment analyses with configuration: Workers=14, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'appliances_timing.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='senti_analy_appliances.log', filemode='w')\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "ray.init(address='ray://172.31.28.176:10001')\n",
    "\n",
    "# Set up key parameters\n",
    "model_path = os.path.expanduser('~/.flair/models/sentiment-en-mix-distillbert_4.pt') # Path to the prebuilt sentiment analyzer on each worker\n",
    "\n",
    "# Contains code for sentiment analysis, loops through different system configurations\n",
    "if __name__ == \"__main__\":\n",
    "    configurations = [\n",
    "        {'num_workers': 2, 'cpu_per_worker': 1, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 200},\n",
    "        {'num_workers': 14, 'cpu_per_worker': 1, 'batch_size': 100}       \n",
    "    ]\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    result_df_initialized = False\n",
    "    \n",
    "    for config in configurations:\n",
    "        num_workers = config['num_workers']\n",
    "        cpu_per_worker = config['cpu_per_worker']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        @ray.remote(num_cpus=cpu_per_worker)\n",
    "        class SentimentAnalyzer:\n",
    "            def __init__(self, model_path):\n",
    "                # Initialize logging inside actor\n",
    "                self.logger = logging.getLogger(__name__)\n",
    "                self.logger.setLevel(logging.INFO)\n",
    "                handler = logging.FileHandler('senti_analy_appliances.log')\n",
    "                handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "                self.logger.addHandler(handler)\n",
    "                \n",
    "                self.classifier = TextClassifier.load(model_path)\n",
    "                self.logger.info(\"Model loaded successfully.\")\n",
    "        \n",
    "            def analyze_sentiments(self, texts):\n",
    "                start_time = time.time()\n",
    "                results = []\n",
    "                for text in texts:\n",
    "                    self.logger.info(f\"Processing text: {text[:15]}...\")  # Log first 15 characters of text\n",
    "                    sentence = Sentence(text)\n",
    "                    try:\n",
    "                        self.classifier.predict(sentence)\n",
    "                        if sentence.labels:\n",
    "                            results.append((sentence.text, sentence.labels[0].value, sentence.labels[0].score))\n",
    "                            self.logger.info(\"Processed result.\")\n",
    "                        else:\n",
    "                            results.append((sentence.text, 'No Prediction', 0))\n",
    "                            self.logger.warning(\"No prediction made for sentence.\")\n",
    "                    except Exception as e:\n",
    "                        error_message = str(e)\n",
    "                        results.append((sentence.text, 'Error', error_message))\n",
    "                        logging.error(f\"Error processing text: {text[:15]}..., error: {error_message}\")\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                return results, processing_time, len(texts)\n",
    "        \n",
    "        def preprocess_texts(texts):\n",
    "            # Remove empty strings and strip whitespace\n",
    "            return [text.strip() for text in texts if text.strip()]\n",
    "        \n",
    "        def distribute_analysis(texts, model_path, num_workers, batch_size):\n",
    "            texts = preprocess_texts(texts)\n",
    "            chunk_size = len(texts) // num_workers\n",
    "            analyzers = [SentimentAnalyzer.remote(model_path) for _ in range(num_workers)]\n",
    "            futures = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for i in range(num_workers):\n",
    "                start_index = i * chunk_size\n",
    "                end_index = start_index + chunk_size if i != num_workers - 1 else len(texts)\n",
    "                for j in range(start_index, end_index, batch_size):\n",
    "                    batch = texts[j:j+batch_size]\n",
    "                    futures.append((len(batch), analyzers[i].analyze_sentiments.remote(batch)))\n",
    "        \n",
    "            results = []\n",
    "            time_per_batch = []\n",
    "            for batch_size, future in futures:\n",
    "                batch_results, batch_time, _ = ray.get(future)\n",
    "                results.extend(batch_results)\n",
    "                time_per_batch.append((batch_size, batch_time))\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            average_time_per_text = total_time / len(texts) if texts else 0\n",
    "            return results, total_time, average_time_per_text, time_per_batch, num_workers, len(analyzers) * cpu_per_worker\n",
    "        \n",
    "        # Distribute and process the analysis\n",
    "        results, total_time, average_time_per_text, time_per_batch, _, _ = distribute_analysis(\n",
    "            Appliances_sampled_texts, model_path, num_workers, batch_size\n",
    "        )\n",
    "        \n",
    "        # Save results to DataFrame and CSV only if not done already\n",
    "        if not result_df_initialized:\n",
    "            df = pd.DataFrame(results, columns=['Text', 'Sentiment', 'Confidence'])\n",
    "            df.to_csv('appliances_results.csv', index=False)\n",
    "            result_df_initialized = True\n",
    "        \n",
    "        # Prepare timing data for CSV\n",
    "        timing_data = {\n",
    "            'Total Time (s)': [total_time],\n",
    "            'Average Time per Text (s)': [average_time_per_text],\n",
    "            'Total Workers': [num_workers],\n",
    "            'CPU per Worker': [cpu_per_worker],\n",
    "            'Batch Size': [batch_size]\n",
    "        }\n",
    "        \n",
    "        # Convert time_per_batch into a DataFrame and merge with timing_data\n",
    "        batch_timing_df = pd.DataFrame(time_per_batch, columns=['Batch Size', 'Time per Batch'])\n",
    "        for column, value in timing_data.items():\n",
    "            batch_timing_df[column] = value[0]  # Add the same value to all rows\n",
    "        \n",
    "        # Append to or create the timing CSV\n",
    "        if os.path.exists('appliances_timing.csv'):\n",
    "            with open('appliances_timing.csv', 'a') as f:\n",
    "                batch_timing_df.to_csv(f, header=False, index=False)\n",
    "        else:\n",
    "            batch_timing_df.to_csv('appliances_timing.csv', index=False)\n",
    "\n",
    "        print(f\"Processed {len(results)} sentiment analyses with configuration: Workers={num_workers}, CPUs={cpu_per_worker}, Batch Size={batch_size}\")\n",
    "        print(f\"Timing data appended to 'appliances_timing.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332731b-319e-4ebe-8a51-b6037156124c",
   "metadata": {},
   "source": [
    "### All Beauty Amazon Reviews\n",
    "We continue with the All Beauty sampled data, running the sentiment analysis six times with each system configuration.\n",
    "- A log of events and warnings is kept to ensure we are aware of any trouble when running (`senti_analy_allbeauty.log`)\n",
    "- Timers are set to provide the runtimes for each system configuration (time data saved to `allbeauty_timing.csv`)\n",
    "  - Total time, time per batch, average time per text\n",
    "- Results from the sentiment analysis on each review text are collected (saved to `allbeauty_results.csv`)\n",
    "  - Text content, sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f516de-c8cd-4ba9-8467-70b1e93cbad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 19728 sentiment analyses with configuration: Workers=2, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'allbeauty_timing.csv'.\n",
      "Processed 20213 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=100\n",
      "Timing data appended to 'allbeauty_timing.csv'.\n",
      "Processed 20813 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=200\n",
      "Timing data appended to 'allbeauty_timing.csv'.\n",
      "Processed 20916 sentiment analyses with configuration: Workers=14, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'allbeauty_timing.csv'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='senti_analy_allbeauty.log', filemode='w')\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "ray.init(address='ray://172.31.28.176:10001')\n",
    "\n",
    "# Set up key parameters\n",
    "model_path = os.path.expanduser('~/.flair/models/sentiment-en-mix-distillbert_4.pt') # Path to the prebuilt sentiment analyzer on each worker\n",
    "\n",
    "# Contains code for sentiment analysis, loops through different system configurations\n",
    "if __name__ == \"__main__\":\n",
    "    configurations = [\n",
    "        {'num_workers': 2, 'cpu_per_worker': 1, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 200},\n",
    "        {'num_workers': 14, 'cpu_per_worker': 1, 'batch_size': 100}       \n",
    "    ]\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    result_df_initialized = False\n",
    "    \n",
    "    for config in configurations:\n",
    "        num_workers = config['num_workers']\n",
    "        cpu_per_worker = config['cpu_per_worker']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        @ray.remote(num_cpus=cpu_per_worker)\n",
    "        class SentimentAnalyzer:\n",
    "            def __init__(self, model_path):\n",
    "                # Initialize logging inside actor\n",
    "                self.logger = logging.getLogger(__name__)\n",
    "                self.logger.setLevel(logging.INFO)\n",
    "                handler = logging.FileHandler('senti_analy_allbeauty.log')\n",
    "                handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "                self.logger.addHandler(handler)\n",
    "                \n",
    "                self.classifier = TextClassifier.load(model_path)\n",
    "                self.logger.info(\"Model loaded successfully.\")\n",
    "        \n",
    "            def analyze_sentiments(self, texts):\n",
    "                start_time = time.time()\n",
    "                results = []\n",
    "                for text in texts:\n",
    "                    self.logger.info(f\"Processing text: {text[:15]}...\")  # Log first 15 characters of text\n",
    "                    sentence = Sentence(text)\n",
    "                    try:\n",
    "                        self.classifier.predict(sentence)\n",
    "                        if sentence.labels:\n",
    "                            results.append((sentence.text, sentence.labels[0].value, sentence.labels[0].score))\n",
    "                            self.logger.info(\"Processed result.\")\n",
    "                        else:\n",
    "                            results.append((sentence.text, 'No Prediction', 0))\n",
    "                            self.logger.warning(\"No prediction made for sentence.\")\n",
    "                    except Exception as e:\n",
    "                        error_message = str(e)\n",
    "                        results.append((sentence.text, 'Error', error_message))\n",
    "                        logging.error(f\"Error processing text: {text[:15]}..., error: {error_message}\")\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                return results, processing_time, len(texts)\n",
    "        \n",
    "        def preprocess_texts(texts):\n",
    "            # Remove empty strings and strip whitespace\n",
    "            return [text.strip() for text in texts if text.strip()]\n",
    "        \n",
    "        def distribute_analysis(texts, model_path, num_workers, batch_size):\n",
    "            texts = preprocess_texts(texts)\n",
    "            chunk_size = len(texts) // num_workers\n",
    "            analyzers = [SentimentAnalyzer.remote(model_path) for _ in range(num_workers)]\n",
    "            futures = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for i in range(num_workers):\n",
    "                start_index = i * chunk_size\n",
    "                end_index = start_index + chunk_size if i != num_workers - 1 else len(texts)\n",
    "                for j in range(start_index, end_index, batch_size):\n",
    "                    batch = texts[j:j+batch_size]\n",
    "                    futures.append((len(batch), analyzers[i].analyze_sentiments.remote(batch)))\n",
    "        \n",
    "            results = []\n",
    "            time_per_batch = []\n",
    "            for batch_size, future in futures:\n",
    "                batch_results, batch_time, _ = ray.get(future)\n",
    "                results.extend(batch_results)\n",
    "                time_per_batch.append((batch_size, batch_time))\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            average_time_per_text = total_time / len(texts) if texts else 0\n",
    "            return results, total_time, average_time_per_text, time_per_batch, num_workers, len(analyzers) * cpu_per_worker\n",
    "        \n",
    "        # Distribute and process the analysis\n",
    "        results, total_time, average_time_per_text, time_per_batch, _, _ = distribute_analysis(\n",
    "            All_Beauty_sampled_texts, model_path, num_workers, batch_size\n",
    "        )\n",
    "        \n",
    "        # Save results to DataFrame and CSV only if not done already\n",
    "        if not result_df_initialized:\n",
    "            df = pd.DataFrame(results, columns=['Text', 'Sentiment', 'Confidence'])\n",
    "            df.to_csv('allbeauty_results.csv', index=False)\n",
    "            result_df_initialized = True\n",
    "        \n",
    "        # Prepare timing data for CSV\n",
    "        timing_data = {\n",
    "            'Total Time (s)': [total_time],\n",
    "            'Average Time per Text (s)': [average_time_per_text],\n",
    "            'Total Workers': [num_workers],\n",
    "            'CPU per Worker': [cpu_per_worker],\n",
    "            'Batch Size': [batch_size]\n",
    "        }\n",
    "        \n",
    "        # Convert time_per_batch into a DataFrame and merge with timing_data\n",
    "        batch_timing_df = pd.DataFrame(time_per_batch, columns=['Batch Size', 'Time per Batch'])\n",
    "        for column, value in timing_data.items():\n",
    "            batch_timing_df[column] = value[0]  # Add the same value to all rows\n",
    "        \n",
    "        # Append to or create the timing CSV\n",
    "        if os.path.exists('allbeauty_timing.csv'):\n",
    "            with open('allbeauty_timing.csv', 'a') as f:\n",
    "                batch_timing_df.to_csv(f, header=False, index=False)\n",
    "        else:\n",
    "            batch_timing_df.to_csv('allbeauty_timing.csv', index=False)\n",
    "\n",
    "        print(f\"Processed {len(results)} sentiment analyses with configuration: Workers={num_workers}, CPUs={cpu_per_worker}, Batch Size={batch_size}\")\n",
    "        print(f\"Timing data appended to 'allbeauty_timing.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa14dbd-ded8-4cb8-a9d0-a16f71ccaea8",
   "metadata": {},
   "source": [
    "### Musical Instruments Amazon Reviews\n",
    "We continue with the Musical Instruments sampled data, running the sentiment analysis six times with each system configuration.\n",
    "- A log of events and warnings is kept to ensure we are aware of any trouble when running (`senti_analy_musicalinstruments.log`)\n",
    "- Timers are set to provide the runtimes for each system configuration (time data saved to `musicalinstruments_timing.csv`)\n",
    "  - Total time, time per batch, average time per text\n",
    "- Results from the sentiment analysis on each review text are collected (saved to `musicalinstruments_results.csv`)\n",
    "  - Text content, sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a42348-aae2-4f1d-a6a9-8d5c76e70076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/ray/util/client/worker.py:619: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 86336 sentiment analyses with configuration: Workers=2, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'musicalinstruments_timing.csv'.\n",
      "Processed 86727 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=100\n",
      "Timing data appended to 'musicalinstruments_timing.csv'.\n",
      "Processed 86727 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=200\n",
      "Timing data appended to 'musicalinstruments_timing.csv'.\n",
      "Processed 86765 sentiment analyses with configuration: Workers=14, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'musicalinstruments_timing.csv'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='senti_analy_musicalinstruments.log', filemode='w')\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "ray.init(address='ray://172.31.28.176:10001')\n",
    "\n",
    "# Set up key parameters\n",
    "model_path = os.path.expanduser('~/.flair/models/sentiment-en-mix-distillbert_4.pt') # Path to the prebuilt sentiment analyzer on each worker\n",
    "\n",
    "# Contains code for sentiment analysis, loops through different system configurations\n",
    "if __name__ == \"__main__\":\n",
    "    configurations = [\n",
    "        {'num_workers': 2, 'cpu_per_worker': 1, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 200},\n",
    "        {'num_workers': 14, 'cpu_per_worker': 1, 'batch_size': 100}       \n",
    "    ]\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    result_df_initialized = False\n",
    "    \n",
    "    for config in configurations:\n",
    "        num_workers = config['num_workers']\n",
    "        cpu_per_worker = config['cpu_per_worker']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        @ray.remote(num_cpus=cpu_per_worker)\n",
    "        class SentimentAnalyzer:\n",
    "            def __init__(self, model_path):\n",
    "                # Initialize logging inside actor\n",
    "                self.logger = logging.getLogger(__name__)\n",
    "                self.logger.setLevel(logging.INFO)\n",
    "                handler = logging.FileHandler('senti_analy_musicalinstruments.log')\n",
    "                handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "                self.logger.addHandler(handler)\n",
    "                \n",
    "                self.classifier = TextClassifier.load(model_path)\n",
    "                self.logger.info(\"Model loaded successfully.\")\n",
    "        \n",
    "            def analyze_sentiments(self, texts):\n",
    "                start_time = time.time()\n",
    "                results = []\n",
    "                for text in texts:\n",
    "                    self.logger.info(f\"Processing text: {text[:15]}...\")  # Log first 15 characters of text\n",
    "                    sentence = Sentence(text)\n",
    "                    try:\n",
    "                        self.classifier.predict(sentence)\n",
    "                        if sentence.labels:\n",
    "                            results.append((sentence.text, sentence.labels[0].value, sentence.labels[0].score))\n",
    "                            self.logger.info(\"Processed result.\")\n",
    "                        else:\n",
    "                            results.append((sentence.text, 'No Prediction', 0))\n",
    "                            self.logger.warning(\"No prediction made for sentence.\")\n",
    "                    except Exception as e:\n",
    "                        error_message = str(e)\n",
    "                        results.append((sentence.text, 'Error', error_message))\n",
    "                        logging.error(f\"Error processing text: {text[:15]}..., error: {error_message}\")\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                return results, processing_time, len(texts)\n",
    "        \n",
    "        def preprocess_texts(texts):\n",
    "            # Remove empty strings and strip whitespace\n",
    "            return [text.strip() for text in texts if text.strip()]\n",
    "        \n",
    "        def distribute_analysis(texts, model_path, num_workers, batch_size):\n",
    "            texts = preprocess_texts(texts)\n",
    "            chunk_size = len(texts) // num_workers\n",
    "            analyzers = [SentimentAnalyzer.remote(model_path) for _ in range(num_workers)]\n",
    "            futures = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for i in range(num_workers):\n",
    "                start_index = i * chunk_size\n",
    "                end_index = start_index + chunk_size if i != num_workers - 1 else len(texts)\n",
    "                for j in range(start_index, end_index, batch_size):\n",
    "                    batch = texts[j:j+batch_size]\n",
    "                    futures.append((len(batch), analyzers[i].analyze_sentiments.remote(batch)))\n",
    "        \n",
    "            results = []\n",
    "            time_per_batch = []\n",
    "            for batch_size, future in futures:\n",
    "                batch_results, batch_time, _ = ray.get(future)\n",
    "                results.extend(batch_results)\n",
    "                time_per_batch.append((batch_size, batch_time))\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            average_time_per_text = total_time / len(texts) if texts else 0\n",
    "            return results, total_time, average_time_per_text, time_per_batch, num_workers, len(analyzers) * cpu_per_worker\n",
    "        \n",
    "        # Distribute and process the analysis\n",
    "        results, total_time, average_time_per_text, time_per_batch, _, _ = distribute_analysis(\n",
    "            Musical_Instruments_sampled_texts, model_path, num_workers, batch_size\n",
    "        )\n",
    "        \n",
    "        # Save results to DataFrame and CSV only if not done already\n",
    "        if not result_df_initialized:\n",
    "            df = pd.DataFrame(results, columns=['Text', 'Sentiment', 'Confidence'])\n",
    "            df.to_csv('musicalinstruments_results.csv', index=False)\n",
    "            result_df_initialized = True\n",
    "        \n",
    "        # Prepare timing data for CSV\n",
    "        timing_data = {\n",
    "            'Total Time (s)': [total_time],\n",
    "            'Average Time per Text (s)': [average_time_per_text],\n",
    "            'Total Workers': [num_workers],\n",
    "            'CPU per Worker': [cpu_per_worker],\n",
    "            'Batch Size': [batch_size]\n",
    "        }\n",
    "        \n",
    "        # Convert time_per_batch into a DataFrame and merge with timing_data\n",
    "        batch_timing_df = pd.DataFrame(time_per_batch, columns=['Batch Size', 'Time per Batch'])\n",
    "        for column, value in timing_data.items():\n",
    "            batch_timing_df[column] = value[0]  # Add the same value to all rows\n",
    "        \n",
    "        # Append to or create the timing CSV\n",
    "        if os.path.exists('musicalinstruments_timing.csv'):\n",
    "            with open('musicalinstruments_timing.csv', 'a') as f:\n",
    "                batch_timing_df.to_csv(f, header=False, index=False)\n",
    "        else:\n",
    "            batch_timing_df.to_csv('musicalinstruments_timing.csv', index=False)\n",
    "\n",
    "        print(f\"Processed {len(results)} sentiment analyses with configuration: Workers={num_workers}, CPUs={cpu_per_worker}, Batch Size={batch_size}\")\n",
    "        print(f\"Timing data appended to 'musicalinstruments_timing.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d245b-b972-4cec-9fc1-291abc122097",
   "metadata": {},
   "source": [
    "### Video Games Amazon Reviews\n",
    "We continue with the Video Games sampled data, running the sentiment analysis six times with each system configuration.\n",
    "- A log of events and warnings is kept to ensure we are aware of any trouble when running (`senti_analy_videogames.log`)\n",
    "- Timers are set to provide the runtimes for each system configuration (time data saved to `videogames_timing.csv`)\n",
    "  - Total time, time per batch, average time per text\n",
    "- Results from the sentiment analysis on each review text are collected (saved to `videogames_results.csv`)\n",
    "  - Text content, sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e76cc2-8ad7-4002-a859-856ad105e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 68555 sentiment analyses with configuration: Workers=2, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'videogames_timing.csv'.\n",
      "Processed 68588 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=100\n",
      "Timing data appended to 'videogames_timing.csv'.\n",
      "Processed 68588 sentiment analyses with configuration: Workers=7, CPUs=2, Batch Size=200\n",
      "Timing data appended to 'videogames_timing.csv'.\n",
      "Processed 68601 sentiment analyses with configuration: Workers=14, CPUs=1, Batch Size=100\n",
      "Timing data appended to 'videogames_timing.csv'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename='senti_analy_videogames.log', filemode='w')\n",
    "\n",
    "# Ensure Ray is shut down and then reinitialize\n",
    "ray.shutdown()\n",
    "ray.init(address='ray://172.31.28.176:10001')\n",
    "\n",
    "# Set up key parameters\n",
    "model_path = os.path.expanduser('~/.flair/models/sentiment-en-mix-distillbert_4.pt') # Path to the prebuilt sentiment analyzer on each worker\n",
    "\n",
    "# Contains code for sentiment analysis, loops through different system configurations\n",
    "if __name__ == \"__main__\":\n",
    "    configurations = [\n",
    "        {'num_workers': 2, 'cpu_per_worker': 1, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 100},\n",
    "        {'num_workers': 7, 'cpu_per_worker': 2, 'batch_size': 200},\n",
    "        {'num_workers': 14, 'cpu_per_worker': 1, 'batch_size': 100}       \n",
    "    ]\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    result_df_initialized = False\n",
    "    \n",
    "    for config in configurations:\n",
    "        num_workers = config['num_workers']\n",
    "        cpu_per_worker = config['cpu_per_worker']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        @ray.remote(num_cpus=cpu_per_worker)\n",
    "        class SentimentAnalyzer:\n",
    "            def __init__(self, model_path):\n",
    "                # Initialize logging inside actor\n",
    "                self.logger = logging.getLogger(__name__)\n",
    "                self.logger.setLevel(logging.INFO)\n",
    "                handler = logging.FileHandler('senti_analy_videogames.log')\n",
    "                handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "                self.logger.addHandler(handler)\n",
    "                \n",
    "                self.classifier = TextClassifier.load(model_path)\n",
    "                self.logger.info(\"Model loaded successfully.\")\n",
    "        \n",
    "            def analyze_sentiments(self, texts):\n",
    "                start_time = time.time()\n",
    "                results = []\n",
    "                for text in texts:\n",
    "                    self.logger.info(f\"Processing text: {text[:15]}...\")  # Log first 15 characters of text\n",
    "                    sentence = Sentence(text)\n",
    "                    try:\n",
    "                        self.classifier.predict(sentence)\n",
    "                        if sentence.labels:\n",
    "                            results.append((sentence.text, sentence.labels[0].value, sentence.labels[0].score))\n",
    "                            self.logger.info(\"Processed result.\")\n",
    "                        else:\n",
    "                            results.append((sentence.text, 'No Prediction', 0))\n",
    "                            self.logger.warning(\"No prediction made for sentence.\")\n",
    "                    except Exception as e:\n",
    "                        error_message = str(e)\n",
    "                        results.append((sentence.text, 'Error', error_message))\n",
    "                        logging.error(f\"Error processing text: {text[:15]}..., error: {error_message}\")\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                return results, processing_time, len(texts)\n",
    "        \n",
    "        def preprocess_texts(texts):\n",
    "            # Remove empty strings and strip whitespace\n",
    "            return [text.strip() for text in texts if text.strip()]\n",
    "        \n",
    "        def distribute_analysis(texts, model_path, num_workers, batch_size):\n",
    "            texts = preprocess_texts(texts)\n",
    "            chunk_size = len(texts) // num_workers\n",
    "            analyzers = [SentimentAnalyzer.remote(model_path) for _ in range(num_workers)]\n",
    "            futures = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for i in range(num_workers):\n",
    "                start_index = i * chunk_size\n",
    "                end_index = start_index + chunk_size if i != num_workers - 1 else len(texts)\n",
    "                for j in range(start_index, end_index, batch_size):\n",
    "                    batch = texts[j:j+batch_size]\n",
    "                    futures.append((len(batch), analyzers[i].analyze_sentiments.remote(batch)))\n",
    "        \n",
    "            results = []\n",
    "            time_per_batch = []\n",
    "            for batch_size, future in futures:\n",
    "                batch_results, batch_time, _ = ray.get(future)\n",
    "                results.extend(batch_results)\n",
    "                time_per_batch.append((batch_size, batch_time))\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            average_time_per_text = total_time / len(texts) if texts else 0\n",
    "            return results, total_time, average_time_per_text, time_per_batch, num_workers, len(analyzers) * cpu_per_worker\n",
    "        \n",
    "        # Distribute and process the analysis\n",
    "        results, total_time, average_time_per_text, time_per_batch, _, _ = distribute_analysis(\n",
    "            Video_Games_sampled_texts, model_path, num_workers, batch_size\n",
    "        )\n",
    "        \n",
    "        # Save results to DataFrame and CSV only if not done already\n",
    "        if not result_df_initialized:\n",
    "            df = pd.DataFrame(results, columns=['Text', 'Sentiment', 'Confidence'])\n",
    "            df.to_csv('videogames_results.csv', index=False)\n",
    "            result_df_initialized = True\n",
    "        \n",
    "        # Prepare timing data for CSV\n",
    "        timing_data = {\n",
    "            'Total Time (s)': [total_time],\n",
    "            'Average Time per Text (s)': [average_time_per_text],\n",
    "            'Total Workers': [num_workers],\n",
    "            'CPU per Worker': [cpu_per_worker],\n",
    "            'Batch Size': [batch_size]\n",
    "        }\n",
    "        \n",
    "        # Convert time_per_batch into a DataFrame and merge with timing_data\n",
    "        batch_timing_df = pd.DataFrame(time_per_batch, columns=['Batch Size', 'Time per Batch'])\n",
    "        for column, value in timing_data.items():\n",
    "            batch_timing_df[column] = value[0]  # Add the same value to all rows\n",
    "        \n",
    "        # Append to or create the timing CSV\n",
    "        if os.path.exists('videogames_timing.csv'):\n",
    "            with open('videogames_timing.csv', 'a') as f:\n",
    "                batch_timing_df.to_csv(f, header=False, index=False)\n",
    "        else:\n",
    "            batch_timing_df.to_csv('videogames_timing.csv', index=False)\n",
    "\n",
    "        print(f\"Processed {len(results)} sentiment analyses with configuration: Workers={num_workers}, CPUs={cpu_per_worker}, Batch Size={batch_size}\")\n",
    "        print(f\"Timing data appended to 'videogames_timing.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650b3b73-e3c8-4e37-9996-58bbea6cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c432e12-25e5-45fd-adc1-9bafc3ccf64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
